---
title: "Working with Text"
description: This blog post will walk you through how I was able to take a pdf version of Harry Potter and the Sorcerers Stone and plot the top five words per chapter and perform a sentiment analysis.
author:
  - name: Annika Leiby
    url: {}
date: 03-10-2021
output:
  distill::distill_article:
    code_folding: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Attach Packages 

library(tidyverse)
library(tidytext)
library(textdata)
library(pdftools)
library(ggwordcloud)
```


### Data Source:

The analysis for this task comes from a pdf version of the first book in the Harry Potter series titled Harry Potter and the Sorcerers Stone. The pdf comes from getfreestories.weebly.com. and can be accessed at http://www.getfreestories.weebly.com/uploads/7/9/0/2/79020522/harry_potter_and_the_sorcerers_-_j.k._rowling.pdf. 


### Part 1: Import text of your choosing 

I imported a pdf version of Harry Potter and the Sorcerers Stone, the first book in J.K. Rowlings series Harry Potter. 

```{r}
# Read in the Harry Potter pdf using the function pdf_text()

harry_potter_text <- pdf_text("hp_sorcerers_stone.pdf")

# Look at it run View(harry_potter_text) in console
# Look at a random page to see what symbol is breaking up the lines 
# Can see that it is "\n"

# hp_text_p39 <- harry_potter_text[39]

# hp_text_p39
```

### Part 2: Wrangle the data to get tokens into tidy format and remove stop words. 

```{r}
# Need to get it into tidy format 
# First convert to a data frame.
# Then mutate a new column called text_full where you use the str_split function on column harry_potter_text to split at the pattern "\n" have to put an extra "\" in front
# Use the un nest function on the text_full column to make each line have its own row 
# Remove excess white space with str_trim function

harry_potter_tidy <- data.frame(harry_potter_text) %>%
  mutate(text_full = str_split(harry_potter_text, pattern = "\\n")) %>%
  unnest(text_full) %>%
  mutate(text_full = str_trim(text_full))


```

```{r}
# Close, but for true tidy format we want each row to have its own word
# Notice there are 44 rows until you get to "Chapter 1"
# Use slice(-(1:44)) to remove rows 1-44

# Want to group by chapter so that can do analysis by chapter 
# Use string detect to detect the chapter number in the column text_full 
# If chapter does appear then repeat it but if it does not than make it an n/a
# Have to give the NA a behind the scenes class of character 
# Then use fill() to fill all na until it gets to the next non na value

harry_potter_df <- harry_potter_tidy %>%
  slice(-(1:44)) %>%
  mutate(chapter = case_when(
    str_detect(text_full, pattern = "CHAPTER") ~ text_full, TRUE ~ NA_character_)) %>%
  fill(chapter) %>%
  separate(col = chapter, into = c("ch", "no"), sep = " ")

```

```{r}
# Now to get it in tokenized text format with each token as a single word
# Use unnest_tokens() function to split an existing column into tokens
# With the new column called word and coming from text_full
# To get rid of the original harry_potter_text column use select(- harry_potter_text)

harry_potter_tokens <- harry_potter_df %>%
  unnest_tokens (word, text_full) %>%
  dplyr::select(-harry_potter_text)

```


### Part 3: Find counts and make a column graph visualization of counts for most frequently used words in text by chapter. 

```{r}
# Now to get word counts by chapter

harry_potter_wordcount <- harry_potter_tokens %>%
  count(no, word)

# harry_potter_wordcount

# Remove the stop words like "a" "the"
# use the function anti_join() to do this 

harry_potter_nonstop_words <- harry_potter_tokens %>%
  anti_join(stop_words) 

nonstop_counts <- harry_potter_nonstop_words %>%
  count(no, word)

# nonstop_counts

```


```{r}
# Find top 5 words by chapter 
# Group by chapter number "no" and arrange from highest count words to lowest count words using arrange() function
# Use the slice() function to only get the top 5 words for each chapter
# Filter our main character names and filler words

top_5_words <- nonstop_counts %>%
  filter(!word %in% c("ron","harry", "hermione", "potter", "hagrid", "didn’t", "ver", "veh", "ter", "it’s", "he’s", "professor", "he’d", "vernon", "petunia", "couldn’t", "i’ve", "don’t", "i’m", "yer", "yeh", "snape", "ronan", "don", "boy", "dumbledore", "dudley", "mcgonagall", "looked", "harry’s", "dursleys", "people", "can’t", "told")) %>% 
  group_by(no) %>%
  arrange(-n) %>%
  slice(1:5)
  
#Note, the "'" did not work at first since it is the more curly version one in the text so I copied and pasted the "’" from the data frame itself. 

# Make a column graph of the word counts facet wrapping by chapter 
ggplot(data = top_5_words, aes(x = word, y = n)) +
  geom_col(aes(fill = no), show.legend = FALSE) +
  facet_wrap(~no, scales = "free") +
  coord_flip() +
  labs(x = "Word", y = "Count")

```

### Part 4: Perform a sentiment analysis using the NRC lexicon and make a visualization of the results.

```{r}
# Now for sentiment analysis
# What sentiments seem to be most prevalent in each of these chapters?
# Using the NRC lexicon 

harry_potter_nrc <- harry_potter_nonstop_words %>%
  inner_join(get_sentiments("nrc"))

harry_potter_nrc_counts <- harry_potter_nrc %>%
  count(no, sentiment) 

#ggplot(data = harry_potter_nrc_counts, aes(x #= sentiment, y = n)) +
#  geom_col() +
#  facet_wrap(~no) +
#  coord_flip()


# Using afinn 


harry_potter_afinn <- harry_potter_nonstop_words %>%
  inner_join(get_sentiments("afinn"))

# Get counts for each score for every chapter 

afinn_counts <- harry_potter_afinn %>%
  count(no, value)

# Getting a mean value for sentiment can be useful 

afinn_means <- harry_potter_afinn %>%
  group_by(no) %>%
  summarize(mean_afinn = mean(value))

ggplot(data = afinn_means, aes(x = no, y = mean_afinn)) +
  geom_col(aes(fill = no), show.legend = FALSE) +
  coord_flip() +
  labs(x = "Chapter", y = "Mean Afinn Value", title = "Mean Afinn Values by Chapter for Harry Potter and Sorcerers Stone")
```

The mean afinn sentiment analysis indicates that all of the chapters except chapters five and thirteen have primarily negative sentiments. Chapters four and fifteen have the highest mean affinn values for sentiment with values between -0.8 and -1.0.