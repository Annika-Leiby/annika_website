[
  {
    "path": "posts/2021-03-10-raster/",
    "title": "Working with Text",
    "description": {},
    "author": [
      {
        "name": "Annika Leiby",
        "url": {}
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\n\n\nShow code\n\n# Attach Packages \n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(pdftools)\nlibrary(ggwordcloud)\n\n\n\nData Source:\nThe analysis for this task comes from a pdf version of the first book in the Harry Potter series titled Harry Potter and the Sorcerers Stone. The pdf comes from getfreestories.weebly.com. and can be accessed at http://www.getfreestories.weebly.com/uploads/7/9/0/2/79020522/harry_potter_and_the_sorcerers_-_j.k._rowling.pdf.\nPart 1: Import text of your choosing\nI imported a pdf version of Harry Potter and the Sorcerers Stone, the first book in J.K. Rowlings series Harry Potter.\n\n\nShow code\n\n# Read in the Harry Potter pdf using the function pdf_text()\n\nharry_potter_text <- pdf_text(\"hp_sorcerers_stone.pdf\")\n\n# Look at it run View(harry_potter_text) in console\n# Look at a random page to see what symbol is breaking up the lines \n# Can see that it is \"\\n\"\n\n# hp_text_p39 <- harry_potter_text[39]\n\n# hp_text_p39\n\n\n\nPart 2: Wrangle the data to get tokens into tidy format and remove stop words.\n\n\nShow code\n\n# Need to get it into tidy format \n# First convert to a data frame.\n# Then mutate a new column called text_full where you use the str_split function on column harry_potter_text to split at the pattern \"\\n\" have to put an extra \"\\\" in front\n# Use the un nest function on the text_full column to make each line have its own row \n# Remove excess white space with str_trim function\n\nharry_potter_tidy <- data.frame(harry_potter_text) %>%\n  mutate(text_full = str_split(harry_potter_text, pattern = \"\\\\n\")) %>%\n  unnest(text_full) %>%\n  mutate(text_full = str_trim(text_full))\n\n\n\n\n\nShow code\n\n# Close, but for true tidy format we want each row to have its own word\n# Notice there are 44 rows until you get to \"Chapter 1\"\n# Use slice(-(1:44)) to remove rows 1-44\n\n# Want to group by chapter so that can do analysis by chapter \n# Use string detect to detect the chapter number in the column text_full \n# If chapter does appear then repeat it but if it does not than make it an n/a\n# Have to give the NA a behind the scenes class of character \n# Then use fill() to fill all na until it gets to the next non na value\n\nharry_potter_df <- harry_potter_tidy %>%\n  slice(-(1:44)) %>%\n  mutate(chapter = case_when(\n    str_detect(text_full, pattern = \"CHAPTER\") ~ text_full, TRUE ~ NA_character_)) %>%\n  fill(chapter) %>%\n  separate(col = chapter, into = c(\"ch\", \"no\"), sep = \" \")\n\n\n\n\n\nShow code\n\n# Now to get it in tokenized text format with each token as a single word\n# Use unnest_tokens() function to split an existing column into tokens\n# With the new column called word and coming from text_full\n# To get rid of the original harry_potter_text column use select(- harry_potter_text)\n\nharry_potter_tokens <- harry_potter_df %>%\n  unnest_tokens (word, text_full) %>%\n  dplyr::select(-harry_potter_text)\n\n\n\nPart 3: Find counts and make a column graph visualization of counts for most frequently used words in text by chapter.\n\n\nShow code\n\n# Now to get word counts by chapter\n\nharry_potter_wordcount <- harry_potter_tokens %>%\n  count(no, word)\n\nharry_potter_wordcount\n\n\n# A tibble: 21,034 x 3\n   no    word        n\n   <chr> <chr>   <int>\n 1 EIGHT 31          1\n 2 EIGHT a          75\n 3 EIGHT able        1\n 4 EIGHT about      10\n 5 EIGHT acid        1\n 6 EIGHT aconite     1\n 7 EIGHT across      2\n 8 EIGHT add         1\n 9 EIGHT added       2\n10 EIGHT afraid      1\n# … with 21,024 more rows\n\nShow code\n\n# Remove the stop words like \"a\" \"the\"\n# use the function anti_join() to do this \n\nharry_potter_nonstop_words <- harry_potter_tokens %>%\n  anti_join(stop_words) \n\nnonstop_counts <- harry_potter_nonstop_words %>%\n  count(no, word)\n\n# nonstop_counts\n\n\n\n\n\nShow code\n\n# Find top 5 words by chapter \n# Group by chapter number \"no\" and arrange from highest count words to lowest count words using arrange() function\n# Use the slice() function to only get the top 5 words for each chapter\n# Filter our main character names and filler words\n\ntop_5_words <- nonstop_counts %>%\n  filter(!word %in% c(\"ron\",\"harry\", \"hermione\", \"potter\", \"hagrid\", \"didn’t\", \"ver\", \"veh\", \"ter\", \"it’s\", \"he’s\", \"professor\", \"he’d\", \"vernon\", \"petunia\", \"couldn’t\", \"i’ve\", \"don’t\", \"i’m\", \"yer\", \"yeh\", \"snape\", \"ronan\", \"don\", \"boy\", \"dumbledore\", \"dudley\", \"mcgonagall\", \"looked\", \"harry’s\", \"dursleys\", \"people\", \"can’t\", \"told\")) %>% \n  group_by(no) %>%\n  arrange(-n) %>%\n  slice(1:5)\n  \n#Note, the \"'\" did not work at first since it is the more curly version one in the text so I copied and pasted the \"’\" from the data frame itself. \n\n# Make a column graph of the word counts facet wrapping by chapter \nggplot(data = top_5_words, aes(x = word, y = n)) +\n  geom_col(aes(fill = no), show.legend = FALSE) +\n  facet_wrap(~no, scales = \"free\") +\n  coord_flip() +\n  labs(x = \"Word\", y = \"Count\")\n\n\n\n\nPart 4: Perform a sentiment analysis using the NRC lexicon and make a visualization of the results.\n\n\nShow code\n\n# Now for sentiment analysis\n# What sentiments seem to be most prevalent in each of these chapters?\n# Using the NRC lexicon \n\nharry_potter_nrc <- harry_potter_nonstop_words %>%\n  inner_join(get_sentiments(\"nrc\"))\n\nharry_potter_nrc_counts <- harry_potter_nrc %>%\n  count(no, sentiment) \n\n#ggplot(data = harry_potter_nrc_counts, aes(x #= sentiment, y = n)) +\n#  geom_col() +\n#  facet_wrap(~no) +\n#  coord_flip()\n\n\n# Using afinn \n\n\nharry_potter_afinn <- harry_potter_nonstop_words %>%\n  inner_join(get_sentiments(\"afinn\"))\n\n# Get counts for each score for every chapter \n\nafinn_counts <- harry_potter_afinn %>%\n  count(no, value)\n\n# Getting a mean value for sentiment can be useful \n\nafinn_means <- harry_potter_afinn %>%\n  group_by(no) %>%\n  summarize(mean_afinn = mean(value))\n\nggplot(data = afinn_means, aes(x = no, y = mean_afinn)) +\n  geom_col(aes(fill = no), show.legend = FALSE) +\n  coord_flip() +\n  labs(x = \"Chapter\", y = \"Mean Afinn Value\", title = \"Mean Afinn Values by Chapter for Harry Potter and Sorcerers Stone\")\n\n\n\n\nThe mean afinn sentiment analysis indicates that all of the chapters except chapters five and thirteen have primarily negative sentiments. Chapters four and fifteen have the highest mean affinn values for sentiment with values between -0.8 and -1.0.\n\n\n\n",
    "preview": "posts/2021-03-10-raster/distill-preview.png",
    "last_modified": "2021-03-10T21:41:49-08:00",
    "input_file": {}
  }
]
